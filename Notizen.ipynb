{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notizen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vor 22.04.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementiert\n",
    "- IMP (aus dem originalen LT-Paper)\n",
    "- Pruning-Methoden\n",
    "    - Magnitude\n",
    "    - Delta-Bewegung high/low (\"Gradient\")\n",
    "- Neuronen-Pruning\n",
    "    - Lösche alle Neuronen bei denen die Inputgewichte alle 0 sind (d.h. die Zeilen der Gewichtsmatrizen)\n",
    "    - Vorteil: Trainingszeit wird kürzer da Netzwerk kleiner wird\n",
    "    - Nachteil: Deutlich höherer Trainingsaufwand\n",
    "    - **Zu Zeigen**: Vorteil von der Pruning-Methode im Vergleich zu einem Netz das direkt in der Größe initialisiert wurde\n",
    "- Pruning-Consitency: Prüfe ob bei gleicher Initialisierung die selben Gewichte \"genullt\" werden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beobachtungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IMP\n",
    "    - Ergebnisse konnten reproduziert werden\n",
    "    - Delta high scheint besser zu funktionieren als Magnitude\n",
    "    - Muster mit Delta deutlich besser erkennbar und deutlich mehr Zeilenvektoren == 0\n",
    "        - Beziehung zwischen Verhalten und Methode??\n",
    "        - Prüfe weitere Methoden!!\n",
    "- Neuronen-Pruning funktioniert prinzipiell. Noch nicht gezeigt dass es besser als direkte Initialisierung ist\n",
    "- Es wird nicht konsistent gepruned!\n",
    "    - Frage: Warum?? Gleiche Daten, Reihenfolge, Initialisierung und trotzdem unterschiedliches Ergebnis..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weitere Pruning-Methoden (siehe \"Deconstructing LT\"-Paper)\n",
    "- Vergleiche Optimierer (SGD, Adam, RMSProp)\n",
    "    - Performance allgemein\n",
    "    - Verhalten bei vorinitalisierten Gewichten\n",
    "- Einfluss/Veränderung des Bias\n",
    "- Untersuche Beziehung zwischen Gewichtsinitialisierung und \"Lottery Ticket\"\n",
    "    - Welchen Einfluss hat die Initialisierung bei LTs?\n",
    "- Weitere Datensätze\n",
    "- Warum werden Gewichte genullt, die sich weniger bewegen?\n",
    "\n",
    "- Bias bei gestrichenen Neuronen? Größe? Verähltnis Bias zu incoming Gewichten\n",
    "- Training Mask Backprop / other optimization techniques\n",
    "- Weight freezing instead of masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benni Anmerkungen\n",
    "\n",
    "- Durch Maske wird \"Cap\" erreicht, der ohne Training der Gewichte nicht überwunden werden kann --> Maskiere erst, und trainiere dann nach\n",
    "- Siehe Sachen auf Whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
